{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfaab626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "# https://drive.google.com/file/d/192jeGRTCZZfet8ufHPfaMn05T7Biklfw/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c46b287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a1a3e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "\n",
    "# Assumes you're in the root level of the dataset directory.\n",
    "# If you aren't, you'll need to change the relative paths here.\n",
    "train_data = text_dataset_from_directory(\"movie-reviews-dataset/test\")\n",
    "test_data = text_dataset_from_directory(\"/movie-reviews-dataset\\\\train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2e2dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from tensorflow.strings import regex_replace\n",
    "\n",
    "def prepareData(dir):\n",
    "  data = text_dataset_from_directory(dir)\n",
    "  return data.map(\n",
    "    lambda text, label: (regex_replace(text, '<br />', ' '), label),\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d21a78c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = prepareData(\"E:\\\\rnn_c2c\\\\movie-reviews-dataset\\\\test\")\n",
    "test_data = prepareData(\"E:\\\\rnn_c2c\\\\movie-reviews-dataset\\\\train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54add890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Although this was a film of only less than forty minutes, it is one of best directed and acted stories I have ever seen. It accomplishes in less than 45 minutes what most films cannot in more than 90.  It is the story of two brothers, one 18 and the other 10. They come from a poor farm family in Mississippi. Both are caught up in war and the conflict of duty verses love of family.  It brought tears to my eyes especially because the entire film is so well acted and directed, plus it tells the story of so many wars where one serves and the other left behind.  I can fully recommend this film as beyond superb !'\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_data.take(1):\n",
    "    print(text_batch.numpy()[0])\n",
    "    print(label_batch.numpy()[0]) # 0 = negative, 1 = positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bedf71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(1,), dtype=\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ead117d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "max_tokens = 1000\n",
    "max_len = 100\n",
    "vectorize_layer = TextVectorization(\n",
    "  # Max vocab size. Any words outside of the max_tokens most common ones\n",
    "  # will be treated the same way: as \"out of vocabulary\" (OOV) tokens.\n",
    "  max_tokens=max_tokens,\n",
    "  # Output integer indices, one per string token\n",
    "  output_mode=\"int\",\n",
    "  # Always pad or truncate to exactly this many tokens\n",
    "  output_sequence_length=max_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d78d5bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call adapt(), which fits the TextVectorization layer to our text dataset.\n",
    "# This is when the max_tokens most common words (i.e. the vocabulary) are selected.\n",
    "train_texts = train_data.map(lambda text, label: text)\n",
    "vectorize_layer.adapt(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "532ddcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(vectorize_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "061ffe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "model.add(Embedding(max_tokens + 1, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5420e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "model.add(LSTM(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd3c63f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, 100)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 128)          128128    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 64, 128)           128128    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 359,297\n",
      "Trainable params: 359,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Embedding(max_tokens + 1, 128))\n",
    "\n",
    "# ----- 4. RECURRENT LAYER\n",
    "model.add(LSTM(64))\n",
    "\n",
    "# ----- 5. DENSE HIDDEN LAYER\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "# ----- 6. OUTPUT\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bd64ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import SimpleRNN\n",
    "# # build model\n",
    "# model.add(SimpleRNN(128, return_sequences=True))\n",
    "# # model.add(SimpleRNN(128, return_sequences=True))\n",
    "# model.add(SimpleRNN(128, return_sequences=False))\n",
    "# model.add(Dense(20))\n",
    "# model.add(Dense(64, activation=\"relu\"))\n",
    "# model.add(Dense(1, activation=\"sigmoid\"))\n",
    "# model.build()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9be2d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(Dense(64, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "663cb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ab9e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760c269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding/embeddings:0', 'lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding/embeddings:0', 'lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "782/782 [==============================] - 74s 84ms/step - loss: 0.6936 - accuracy: 0.4992\n",
      "Epoch 2/10\n",
      "356/782 [============>.................] - ETA: 33s - loss: 0.6934 - accuracy: 0.4967"
     ]
    }
   ],
   "source": [
    "model.fit(train_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698ae97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('E:\\\\rnn_c2c\\\\movie-reviews-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b9afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('E:\\\\rnn_c2c\\\\movie-reviews-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7798b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should print a very high score like 0.98.\n",
    "print(model.predict([\n",
    "  \"i loved it! highly recommend it to anyone and everyone looking for a great movie to watch.\",\n",
    "]))\n",
    "\n",
    "# Should print a very low score like 0.01.\n",
    "print(model.predict([\n",
    "  \"this was awful! i hated it so much, nobody should watch this. the acting was terrible, the music was terrible, overall it was just bad.\",\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7a673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
