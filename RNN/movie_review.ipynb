{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfaab626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "# https://drive.google.com/file/d/192jeGRTCZZfet8ufHPfaMn05T7Biklfw/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c46b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a1a3e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "\n",
    "# Assumes you're in the root level of the dataset directory.\n",
    "# If you aren't, you'll need to change the relative paths here.\n",
    "train_data = text_dataset_from_directory(\"D:\\\\Training\\\\Movie Review\\\\movie-reviews-dataset\\\\train\")\n",
    "test_data = text_dataset_from_directory(\"D:\\\\Training\\\\Movie Review\\\\movie-reviews-dataset\\\\test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2e2dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from tensorflow.strings import regex_replace\n",
    "\n",
    "def prepareData(dir):\n",
    "  data = text_dataset_from_directory(dir)\n",
    "  return data.map(\n",
    "    lambda text, label: (regex_replace(text, '<br />', ' '), label),\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d21a78c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = prepareData(\"D:\\\\Training\\\\Movie Review\\\\movie-reviews-dataset\\\\train\")\n",
    "test_data = prepareData(\"D:\\\\Training\\\\Movie Review\\\\movie-reviews-dataset\\\\test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54add890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'The best film on the battle of San Antonio, Texas in March 1836, was John Wayne\\'s 1960 epic THE ALAMO. In a one shot job as director producer, that temporarily financially strapped him, Wayne demonstrated that he was talented in movie making outside of his icon-like acting ability personifying the West.  I have commented on that film in a review the other night, and I pointed out that Wayne and James Edward Grant (the screenwriter) tackled some points that were barely mentioned in earlier films about the battle. They did bring in the issue of slavery. They also finally discussed the contribution of local Mexican land owner Juan Seguin as an important leader in the War for Independence on par with Crockett, Bowie, Travis, Austin, and Houston.   But there was one weakness (though well hidden) in the film. Wayne worked hard to cast it properly, thinking of many people for lead roles in it. But, he did not properly handle the leader of the enemy forces, General Antonio De Santa Anna. The role was played by an obscure actor, Ruben Padilla (on this board, his thread shows only three credits listed). Padilla did not have any spoken dialog (even in Spanish). And while he does have one of the last shots in the film, he just is shown as a silent tyrant, observing the burning of the bodies of the Americans and their allies.  Despite several poor choices in the casting of this television movie (THE ALAMO: THIRTEEN DAYS TO CLORY), it is the best film in showing the man who was (from 1836 to 1854) a leading bogeyman to American policy makers. Raul Julia was a wonderful stage actor. I was fortunate to see him in a production (in the late 1980s) of ARMS AND THE MAN in Manhattan, as Sergius. He was never boring, and usually first rate in his acting.  Here we see the egotistical monster at his worst. Nothing is acceptable that does not fit Santa Anna\\'s wishes or activities. It can be the failure of an orderly in the army to bring some item he requested fast enough, or it can be the temerity of these \"foreign brigands\" (as he saw the Americans) in not knuckling down to himself, \"the Napoleon of the West\".  Santa Anna was President of Mexico five or six times between 1830 and 1855. He claimed that he first got involved in overthrowing a President because that President did not live up to the country\\'s constitution, but it was the power that kept him going year after year. It is a sad commentary that he was the leading Mexican historical figure in those two decades. No political figure or military figure would rise to override him until Benito Juarez did in the late 1850s. Initially he claimed great liberal ideals, but he once admitted that the people of Mexico were children who needed guidance for one hundred years before they could rule themselves (and thus he sounds like Gilbert Roland in CRISIS talking about the people he has helped lead against Jose Ferrer). The amazing thing about him was he managed to keep coming back. His policies were disasters. While we know about his attack on Texas (to put down a revolt there), he also tried to expand into Guatamala (and probably saw himself controlling much of Central America). He did win at the Alamo, but at great cost of lives. His massacre of Col. Fannin\\'s men at Goliad was inexcusable (one might make a case for the destruction of the defenders of the Alamo who were fighting to the last, but Fannin had surrendered). Then came the disaster of San Jacinto, where his army was wiped out (he failed to take adequate precautions to watch for the American troops). He was captured, and humiliated, and forced to sign a surrender of Texas. Houston was kind to him: the troops wanted to string him up.  Except for losing a leg in a battle against the French in 1838, he managed not to get wounded in most of his wars. He repudiated the forced surrender of Texas, but could not militarily undue it. Instead, he would lead Mexico into defeat in the war of 1846 - 48 against the Americans, leading to the Mexican Session. The U.S. was \"decent\" enough to pay Mexico $15,000,000 for the Southwest, but Mexico lost half of it\\'s territory. He would be President for the last time in 1853, in time to give Franklin Pierce\\'s horrendously bad administration it\\'s one moment of glory - Santa Anna sold the border of Arizona and New Mexico (the \"Gadsden Purchase\") to the U.S. No other Mexican President (not even Porfirio Diaz) ever cost his country so much (Diaz did sell out to foreign business interests, but he built up Mexico\\'s economic muscles doing so). He was exiled in 1855, and settled in Staten Island. There he managed to do his most creative work: he introduced chicle to the U.S., and it became chewing gum. Some achievement!   Julia\\'s Santa Anna is younger than the practiced cynic and schemer who became America\\'s best land purchase agent. He is not going to stand for opposition and he jumps into furious tantrums at a moment\\'s notice. Most of the time his chief aide, Col. Black (David Ogden Stiers, here a British born officer) holds his tongue - he does not wish to be in front of a firing squad as he could be. But Stiers is secretly less than enchanted by his boss. At the end, when alone with the newly widowed wives of the dead Alamo defenders, Stiers suggests that they tell the world what Santa Anna is really like. And they did!'\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_data.take(1):\n",
    "    print(text_batch.numpy()[0])\n",
    "    print(label_batch.numpy()[0]) # 0 = negative, 1 = positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bedf71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(1,), dtype=\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ead117d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "max_tokens = 1000\n",
    "max_len = 100\n",
    "vectorize_layer = TextVectorization(\n",
    "  # Max vocab size. Any words outside of the max_tokens most common ones\n",
    "  # will be treated the same way: as \"out of vocabulary\" (OOV) tokens.\n",
    "  max_tokens=max_tokens,\n",
    "  # Output integer indices, one per string token\n",
    "  output_mode=\"int\",\n",
    "  # Always pad or truncate to exactly this many tokens\n",
    "  output_sequence_length=max_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d78d5bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call adapt(), which fits the TextVectorization layer to our text dataset.\n",
    "# This is when the max_tokens most common words (i.e. the vocabulary) are selected.\n",
    "train_texts = train_data.map(lambda text, label: text)\n",
    "vectorize_layer.adapt(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "532ddcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(vectorize_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "061ffe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "model.add(Embedding(max_tokens + 1, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5420e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "model.add(LSTM(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd3c63f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, 100)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 128)          128128    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 64, 128)           128128    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 359,297\n",
      "Trainable params: 359,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Embedding(max_tokens + 1, 128))\n",
    "\n",
    "# ----- 4. RECURRENT LAYER\n",
    "model.add(LSTM(64))\n",
    "\n",
    "# ----- 5. DENSE HIDDEN LAYER\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "# ----- 6. OUTPUT\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bd64ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import SimpleRNN\n",
    "# # build model\n",
    "# model.add(SimpleRNN(128, return_sequences=True))\n",
    "# # model.add(SimpleRNN(128, return_sequences=True))\n",
    "# model.add(SimpleRNN(128, return_sequences=False))\n",
    "# model.add(Dense(20))\n",
    "# model.add(Dense(64, activation=\"relu\"))\n",
    "# model.add(Dense(1, activation=\"sigmoid\"))\n",
    "# model.build()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9be2d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(Dense(64, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "663cb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ab9e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c760c269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding/embeddings:0', 'lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['embedding/embeddings:0', 'lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "782/782 [==============================] - 77s 91ms/step - loss: 0.6934 - accuracy: 0.5014\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 117s 149ms/step - loss: 0.6934 - accuracy: 0.4971\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 68s 85ms/step - loss: 0.6933 - accuracy: 0.4943\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 41s 52ms/step - loss: 0.6932 - accuracy: 0.4977\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 42s 53ms/step - loss: 0.6932 - accuracy: 0.4961\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 45s 58ms/step - loss: 0.6932 - accuracy: 0.4975\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 121s 154ms/step - loss: 0.6932 - accuracy: 0.4972\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 62s 78ms/step - loss: 0.6932 - accuracy: 0.4987\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 43s 55ms/step - loss: 0.6932 - accuracy: 0.4966\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 92s 116ms/step - loss: 0.6932 - accuracy: 0.4954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18a7a142b50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "698ae97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\Training\\Movie Review\\movie-reviews-dataset\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\Training\\Movie Review\\movie-reviews-dataset\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('D:\\\\Training\\\\Movie Review\\\\movie-reviews-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4b9afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('D:\\\\Training\\\\Movie Review\\\\movie-reviews-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7798b38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "[[0.49607146]]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[[0.49607146]]\n"
     ]
    }
   ],
   "source": [
    "# Should print a very high score like 0.98.\n",
    "print(model.predict([\n",
    "  \"i loved it! highly recommend it to anyone and everyone looking for a great movie to watch.\",\n",
    "]))\n",
    "\n",
    "# Should print a very low score like 0.01.\n",
    "print(model.predict([\n",
    "  \"this was awful! i hated it so much, nobody should watch this. the acting was terrible, the music was terrible, overall it was just bad.\",\n",
    "]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('testproject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "576a1964e18380f436f45d5a973ec26472320b16dd5452d89b1e2d7b99cd141e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
